---
title: "Group Assignment"
author: "Young Kim"
date: "2023-11-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Problem #9: This problem involves the OJ data set which is part of the ISLR package.
(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
```{r}
library(ISLR)

attach(OJ)

fix(OJ)
dim(OJ)

set.seed(3)

train = sample(1:nrow(OJ),535) # without replacement

OJ.test = OJ[-train,]    # test data
 
Purchase.test = Purchase[-train] # target variables corresponding to the test data
```


(b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
```{r}
library(tree)

tree.OJ = tree(Purchase~.-Purchase, OJ, subset=train)

summary(tree.OJ)

plot(tree.OJ)

text(tree.OJ,pretty=0)

# Missclassification error rate: 0.172
# The number of terminal nodes: 10

```
(c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
```{r}
tree.OJ
# selected node 2)
# split: LoyalCH < 0.558034
# the number of observations in this terminal node: 233
# deviance: 288.100
# Y value: Minute Maid
# the probabilities of y value: CH 0.30901, MM 0.69099
```


(d) Create a plot of the tree, and interpret the results.
```{r}
plot(tree.OJ)
text(tree.OJ, pretty=0)
# The resulting model indicates that the significant variables affecting the purchase are:
# the loyalty for CH, the price difference between CH and MM, the store ID, the sale price of MM, the discount on MM
# Among the variables, the loyalty for CH is the most significant one because it appears for many times.
```


(e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
```{r}
tree_pred = predict(tree.OJ, OJ.test, type="class")
table(tree_pred, Purchase.test)

mean(tree_pred != Purchase.test)
# test error rate is 0.21
```


(f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.
```{r}
cv_oj = cv.tree(tree.OJ, FUN=prune.misclass)
```


(g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
```{r}
plot(cv_oj$size, cv_oj$dev, type = "b", xlab = "Tree size", ylab = "Classification Error")
```


(h) Which tree size corresponds to the lowest cross-validated classification error rate?
```{r}
# Accoring to the graph, the size of 2 has the lowest error rate.
```


(i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
```{r}
prune_oj <- prune.misclass(tree.OJ, best = 2)
plot(prune_oj)
text(prune_oj, pretty=0)
```


(j) Compare the training error rates between the pruned and unpruned trees. Which is higher?
```{r}
# the tree model before pruning has a training error rate of .172
summary(prune_oj)

# the tree model after pruning has a training error rate of .2131, which is slightly higher.
```


(k) Compare the test error rates between the pruned and unpruned trees. Which is higher?
```{r}
# The tree model before pruning has test error rate of .21
prune_pred = predict(prune_oj, OJ.test, type="class")
table(prune_pred, Purchase.test)

mean(prune_pred != Purchase.test)
# The tree model after pruning has a test error rate of.22, which is slightly higher.
```

